{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "467a1e5b",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38771d6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547651f6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8df38cb206a0041906d8b97c40b8c61b",
     "grade": false,
     "grade_id": "cell-f73070a62e71039a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f1e292",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3191b8ea8b5fbd41ddbeeaa7879125b2",
     "grade": false,
     "grade_id": "cell-d40d61f7f047b0a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Stochastic gradient Langevin dynamics, and uncertainty quantification\n",
    "\n",
    "In this first part of the coursework you'll do two things:\n",
    "\n",
    "* Implement stochastic gradient Langevin dynamics for sampling from a Bayesian neural network\n",
    "* Break down the variance to estimate epistemic and aleatoric uncertainty\n",
    "\n",
    "We're going to run this on the following toy 2D dataset, consisting of **two moons**, each a different class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, validation_set = torch.load(\"two_moons.pt\")\n",
    "X_train, y_train = dataset.tensors\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='autumn', edgecolor='k');\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072226a",
   "metadata": {},
   "source": [
    "## We'll use the following simple feedforward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95119861",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c08538e37f9d083628542b9d033bea3a",
     "grade": false,
     "grade_id": "cell-0a1ba66fc4e24093",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TwoMoonsNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(2, 100),\n",
    "                        nn.ReLU(), \n",
    "                        nn.Linear(100, 10),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(10, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        return torch.sigmoid(h).squeeze(1)\n",
    "    \n",
    "network = TwoMoonsNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27456df",
   "metadata": {},
   "source": [
    "# TASK 1 (5 points): Define the likelihood and the prior\n",
    "\n",
    "Let $x_i \\in \\mathbb{R}^2$ denote an input, $y_i \\in \\{ 0, 1\\}$ a target, and $\\theta$ the network parameters.\n",
    "\n",
    "Let $\\hat y_i = f_\\theta(x_i)$ denote the output of the network $f_\\theta$.\n",
    "\n",
    "You need to define:\n",
    "\n",
    "* `log_likelihood`, evaluating a Bernoulli distribution $$\\log p(y|x, \\theta) = \\log \\mathrm{Bernoulli}(y | f_\\theta(x));$$\n",
    "* `log_prior`, evaluating a standard normal distribution $$\\log p(\\theta) = \\log \\mathcal{N}(\\theta | 0, I).$$\n",
    "\n",
    "Both of these functions should return a numeric scalar value.\n",
    "\n",
    "For the prior, you will need to somehow access the parameters of the network! There are several ways of doing this. I suggest looking at the pytorch documentation for either\n",
    "\n",
    "* iterating through `network.parameters()`, or\n",
    "* using helper functions such as `nn.utils.parameters_to_vector`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5cafa4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa87b460b8aa5442a268e9ae3cdbae8e",
     "grade": false,
     "grade_id": "two-likelihood",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_likelihood(network, X, y):\n",
    "    \"\"\"\n",
    "    This function computes the log probability `log p(y | x, theta)`\n",
    "    for a batch of inputs X.\n",
    "    \n",
    "    INPUT:\n",
    "    network : instance of classifier network, extends `nn.Module`\n",
    "    X       : batch of inputs; torch.FloatTensor, matrix of shape = (batch_size, 2)\n",
    "    y       : batch of targets: torch.FloatTensor, vector of shape = (batch_size,)\n",
    "    \n",
    "    OUTPUT:\n",
    "    lp      : log probability value of log p(y | x, theta); scalar\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9806f8b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "891757ba46deab783137487e3a2b1523",
     "grade": true,
     "grade_id": "test-likelihood",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4f01d2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06216d634c7ef01cc64d5d4b67c72e04",
     "grade": false,
     "grade_id": "two-prior",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_prior(network):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948dec7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7981d79aefbcdfc88e917632c27c590d",
     "grade": true,
     "grade_id": "test-prior",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77292068",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc0396f1cca6324d203f8292e3f747f9",
     "grade": true,
     "grade_id": "test-prior-likelihood",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf699557",
   "metadata": {},
   "source": [
    "# TASK 2 (3 points): Minibatching\n",
    "\n",
    "Minibatching is \"trickier\" when we have a prior on the network parameters.\n",
    "\n",
    "The function `log_joint_minibatch` should return a mini-batch estimate of the log joint of the full data, i.e. returning an estimator of $$\\log p(\\theta) + \\sum_{i=1}^N \\log p(y_i | x_i, \\theta)$$ but which is evaluated on only the specified subset of the $N$ data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58fa2b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a355d4ca8df882e6d004d33b1c8f8e07",
     "grade": false,
     "grade_id": "two-minibatch",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_joint_minibatch(network, X_batch, y_batch, N_training):\n",
    "    \"\"\" Return a minibatch estimate of the full log joint probability \n",
    "    \n",
    "    INPUT:\n",
    "    network    : instance of classifier network, extends `nn.Module`\n",
    "    X_batch    : batch of inputs; torch.FloatTensor, matrix of shape = (batch_size, 2)\n",
    "    y_batch    : batch of targets: torch.FloatTensor, vector of shape = (batch_size,)\n",
    "    N_training : total number of training data instances in the full training set\n",
    "\n",
    "    OUTPUT:\n",
    "    lp : return an estimate of log p(y, theta | X), as computed on the batch; scalar.\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655c8ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f01e3d24705f09d90f56eecdf37cf6f9",
     "grade": true,
     "grade_id": "test-minibatch-one",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967ce06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "287156b5b88dba202b6b317d02d008d8",
     "grade": true,
     "grade_id": "test-minibatch-two",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17985bc2",
   "metadata": {},
   "source": [
    "## Self-diagnostic\n",
    "\n",
    "This is a small-enough dataset that we can compute the log joint probability on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d9823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    full_data_lp = log_prior(network) + log_likelihood(network, X_train, y_train)\n",
    "print(\"Full data log probability: %0.4f\" % full_data_lp.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecebf58",
   "metadata": {},
   "source": [
    "### In the next cell, we call `log_joint_minibatch`.\n",
    "\n",
    "If all the code is working correctly, the minibatch estimates should be roughly normally-distributed around the true value!\n",
    "\n",
    "Changing the batch size should not change the mean of this distribution. Try running the cell below several times.\n",
    "\n",
    "If the histogram is far away from the true value (the black vertical dashed line), then you probably have a bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fff367",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_lp_est = [log_joint_minibatch(network, X, y, len(dataset)).item() for X, y in dataloader]\n",
    "plt.hist(batch_lp_est, bins=10);\n",
    "yl = plt.ylim()\n",
    "plt.plot(full_data_lp.item()*np.ones(2), [0, plt.ylim()[1]], 'k--')\n",
    "plt.ylim(yl);\n",
    "# torch.stack(batch_lp_est).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d0759",
   "metadata": {},
   "source": [
    "## Train the model (MAP estimation)\n",
    "\n",
    "Run the following cell to train your model, to find a MAP estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=5)\n",
    "opt = torch.optim.Adam(network.parameters())\n",
    "\n",
    "N_epochs = 400\n",
    "train_lp = []\n",
    "\n",
    "for epoch in range(N_epochs):\n",
    "    for X, y in dataloader:\n",
    "        opt.zero_grad()\n",
    "        loss = -log_joint_minibatch(network, X, y, len(dataset))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():\n",
    "        train_lp.append((log_likelihood(network, X_train, y_train) + log_prior(network)).item())\n",
    "plt.plot(train_lp)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"$p(y, \\\\theta | x)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656cc92a",
   "metadata": {},
   "source": [
    "## Confidence plot\n",
    "\n",
    "To visualize the result, we'll look at a *confidence plot*.\n",
    "\n",
    "The *confidence* is the probability assigned to whichever class is predicted.\n",
    "\n",
    "If the most likely class is \"yellow\", then it shows $p(y = \\text{yellow})$. If the most likely class is red, it shows $p(y = \\text{red})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, YY = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-2, 2, 100))\n",
    "with torch.no_grad():\n",
    "    py = network(torch.FloatTensor(np.stack((XX.ravel(), YY.ravel())).T))\n",
    "    conf = torch.where(py > 0.5, py, 1-py) ## THIS LINE COMPUTES THE CONFIDENCE\n",
    "    ZZ = conf.reshape(XX.shape)\n",
    "CS = plt.contourf(XX, YY, ZZ, cmap='Blues', vmin=0.5, vmax=1.2)\n",
    "plt.colorbar(ticks=np.linspace(0.5, 1, 6))\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='autumn', edgecolor='k')\n",
    "plt.title(\"Confidence plot\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c18e1d",
   "metadata": {},
   "source": [
    "# Calibration and reliability diagrams\n",
    "\n",
    "I've implemented a reliability diagram plot and expected calibration error computation here.\n",
    "\n",
    "Could be interesting to see how the number of bins affects the result…!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reliability_diagram(y, y_hat, N_bins=10):\n",
    "    \"\"\"\n",
    "    Plot a reliability diagram\n",
    "    \"\"\"\n",
    "    bins = torch.linspace(0, 1, N_bins+1)\n",
    "    which_bin = (y_hat[:,None] >= bins[1:]).long().sum(-1).numpy()\n",
    "    freq = [y[which_bin==k].mean().item() for k in range(N_bins)]\n",
    "    conf = [y_hat[which_bin==k].mean().item() for k in range(N_bins)]\n",
    "    plt.bar(bins[1]/2+np.arange(N_bins)/N_bins, freq, width=bins[1]*.9);\n",
    "    plt.plot(conf, freq, 'o-', color='k')\n",
    "    plt.plot([0, 1], [0, 1], '--', color='#333');\n",
    "\n",
    "def expected_calibration_error(y, y_hat, N_bins=10):\n",
    "    \"\"\"\n",
    "    Compute the expected calibration error\n",
    "    \"\"\"\n",
    "    bins = torch.linspace(0, 1, N_bins+1)\n",
    "    which_bin = (y_hat[:,None] >= bins[1:]).long().sum(-1).numpy()\n",
    "    counts = []\n",
    "    acc = []\n",
    "    conf = []\n",
    "    for k in range(N_bins):\n",
    "        count = len(y[which_bin==k])\n",
    "        if count > 0:\n",
    "            acc.append(y[which_bin==k].mean().item())\n",
    "            conf.append(y_hat[which_bin==k].mean().item())\n",
    "        else:\n",
    "            acc.append(0.0)\n",
    "            conf.append(0.0)\n",
    "        counts.append(count)\n",
    "    return ((np.abs(np.array(acc) - np.array(conf))*np.array(counts))/len(y)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat = network(validation_set.tensors[0])\n",
    "\n",
    "reliability_diagram(y=validation_set.tensors[1], y_hat=y_hat)\n",
    "print(\"ECE = %0.4f\" % expected_calibration_error(validation_set.tensors[1], y_hat))\n",
    "plt.title(\"Reliability diagram\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6700854",
   "metadata": {},
   "source": [
    "# TASK 3 (6 points): stochastic gradient Langevin dynamics (SGLD)\n",
    "\n",
    "In this task you can get\n",
    "\n",
    "* 3 points for implementing the SGLD MCMC update\n",
    "* 3 points for implementing Monte Carlo prediction\n",
    "\n",
    "The `SGLD_step` function below should take a current set of network parameters $\\theta$, and update them as\n",
    "\n",
    "$$\\theta' = \\theta + \\frac{\\epsilon^2}{2} \\nabla_\\theta \\log p(\\theta, y | X) + \\epsilon z$$\n",
    "\n",
    "where $\\epsilon$ is a learning rate, $X, y$ are a current mini-batch, and $z \\sim \\mathcal{N}(0, I)$ and has the same dimensionality as $\\theta$.\n",
    "\n",
    "For more details see the week 7 lecture slides!\n",
    "\n",
    "Note that you will actually have to **do the update**. The following function does not return anything! Instead, it updates the parameters of the network, similar to how a `torch.optim` optimizer updates the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf2bec",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4be9bffc10ece8efd669f80029225db2",
     "grade": false,
     "grade_id": "sgld-step",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def SGLD_step(network, X, y, N_training, epsilon):\n",
    "    \"\"\"\n",
    "    Run one step of SGLD given a mini-batch, and update the parameters of the network.\n",
    "    \n",
    "    INPUT:\n",
    "    network    : instance of classifier network, extends `nn.Module`\n",
    "    X_batch    : batch of inputs; torch.FloatTensor, matrix of shape = (batch_size, 2)\n",
    "    y_batch    : batch of targets: torch.FloatTensor, vector of shape = (batch_size,)\n",
    "    N_training : total number of training data instances in the full training set\n",
    "    epsilon    : step size / learning rate parameters (scalar)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30501a6f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6aaccb46736bb2481a8f7d095a53b82",
     "grade": true,
     "grade_id": "test-sgld-step-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e0a5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed81a907a6f4d47e0f4eaa713e1405dd",
     "grade": true,
     "grade_id": "test-sgld-step-2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e1b8fb",
   "metadata": {},
   "source": [
    "### Cyclic learning rate \n",
    "\n",
    "We're going to use a cyclic learning rate schedule for $\\epsilon$, like discussed in the lectures. That function is pre-written, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f776c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_schedule(N_steps, N_samples, epsilon):\n",
    "    \"\"\"\n",
    "    Pre-compute a learning-rate schedule for SGLD.\n",
    "    \n",
    "    INPUT:\n",
    "    N_steps   : number of SGD updates between each \"reset\"\n",
    "    N_samples : number of times we reach the lowest target learning rate\n",
    "    epsilon   : base learning rate\n",
    "    \n",
    "    OUTPUT:\n",
    "    epsilon_t : vector of length N_steps*N_samples, containing epsilon_t at each iteration t\n",
    "    \"\"\"\n",
    "    return epsilon * (np.cos(np.pi * (np.arange(N_samples*N_steps) % N_steps)/N_steps) + 1)\n",
    "\n",
    "plt.plot(learning_rate_schedule(N_steps=100, N_samples=10, epsilon=0.5));\n",
    "plt.xlabel(\"iteration\");\n",
    "plt.ylabel(\"epsilon\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85afd3e8",
   "metadata": {},
   "source": [
    "### Running the sampler\n",
    "The following function is also given: it will actually run the SGLD sampler, by calling your `SGLD_step` function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454881a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sgld_samples(network, N_samples, N_steps_per_sample, base_epsilon=0.02):\n",
    "    \"\"\"\n",
    "    Draw samples using SGLD, following a prescribed learning rate schedule\n",
    "    \n",
    "    OUTPUT:\n",
    "    samples : torch.FloatTensor, shape = (N_samples, \"# of parameters in network\")\n",
    "    \"\"\"\n",
    "    lr_schedule = learning_rate_schedule(N_steps_per_sample, N_samples, base_epsilon)\n",
    "    samples = []\n",
    "    step = 0\n",
    "    while True:\n",
    "        for X, y in dataloader:\n",
    "            SGLD_step(network, X, y, len(dataset), epsilon=lr_schedule[step])\n",
    "            step += 1\n",
    "            if step % N_steps_per_sample == 0:\n",
    "                samples.append(nn.utils.parameters_to_vector(network.parameters()).detach())\n",
    "            if step == len(lr_schedule):\n",
    "                return torch.stack(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58babb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = draw_sgld_samples(network, N_samples=50, N_steps_per_sample=200)\n",
    "print(\"Tensor holding samples of theta has shape:\", samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c3ae4",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "After running the sampler, you now have a `torch.FloatTensor` which contains many different sampled versions of the network parameters $s = 1,\\dots,S$.\n",
    "\n",
    "To make predictions on an input batch $X$, you will somehow need to \"load\" in the stored parameters, and run the forward pass using different $\\theta^{(s)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93edb1e5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4580ac588abe402ca474a847915f461",
     "grade": false,
     "grade_id": "sgld-predict",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_from_samples(X, network, samples):\n",
    "    \"\"\"\n",
    "    \n",
    "    INPUT:\n",
    "    X       : batch of input points at which to make predictions; shape = (batch_size, 2)\n",
    "    network : instance of classifier network, extends `nn.Module`\n",
    "    samples : torch.FloatTensor containing samples of theta; shape = (num_samples, \"# of parameters in network\")\n",
    "    \n",
    "    OUTPUT:\n",
    "    y_hat_samples : torch.FloatTensor containing samples of y_hat; shape = (num_samples, batch_size)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9cda3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c10a2a2d756ae48a3117d8572d16a98a",
     "grade": true,
     "grade_id": "samples-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The following should be true if your code is working!\n",
    "assert predict_from_samples(X_train, network, samples).shape == (samples.shape[0], X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab798025",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33567d2eda1633f86b0f5c5518f1fdd5",
     "grade": true,
     "grade_id": "samples-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456a71d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d082bd6b84fd237f342944f6a6319f41",
     "grade": true,
     "grade_id": "samples-3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# grading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5851f0",
   "metadata": {},
   "source": [
    "## Confidence plot and reliability diagrams for the Bayesian classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67219bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, YY = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-2, 2, 100))\n",
    "with torch.no_grad():\n",
    "    XXYY = torch.FloatTensor(np.stack((XX.ravel(), YY.ravel())).T)\n",
    "    out = predict_from_samples(XXYY, network, samples)\n",
    "    py = out.mean(0)\n",
    "    conf = torch.where(py > 0.5, py, 1-py)\n",
    "    ZZ = conf.reshape(XX.shape)\n",
    "CS = plt.contourf(XX, YY, ZZ, cmap='Blues', vmin=0.5, vmax=1.2)\n",
    "plt.colorbar(ticks=np.linspace(0.5, 1, 6))\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='autumn', edgecolor='k');\n",
    "plt.title(\"Confidence plot\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338784ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat = predict_from_samples(validation_set.tensors[0], network, samples).mean(0)\n",
    "\n",
    "reliability_diagram(y=validation_set.tensors[1], y_hat=y_hat)\n",
    "print(\"ECE = %0.4f\" % expected_calibration_error(validation_set.tensors[1], y_hat))\n",
    "\n",
    "plt.title(\"Reliability diagram\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804904c2",
   "metadata": {},
   "source": [
    "## Uncertainty quantification\n",
    "\n",
    "We can measure the uncertainty in our predictor by looking at the variance of the predictive distribution\n",
    "\n",
    "$$p(y | \\hat y).$$\n",
    "\n",
    "Here, $\\hat y$ is the posterior mean: the average $p(y = 1)$ estimated across all posterior samples.\n",
    "\n",
    "The total predictive variance of $y$ is the variance of this distribution, i.e.\n",
    "\n",
    "$$Var [ y | x ] = Var \\left [ \\int p(y | x, \\theta)p(\\theta | \\mathcal{D}) d\\theta \\right]$$\n",
    "\n",
    "One way to estimate epistemic and aleatoric uncertainty is to look at the variance of $\\hat y$, i.e. the variance in $f_\\theta(x)$ under the posterior over $\\theta$. This corresponds to epistemic uncertainty. The difference between these two is then roughly the aleatoric uncertainty.\n",
    "\n",
    "There are probably better ways to compute (and explain!) this, but hopefully the following plots look clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance of predictive distribution over y\n",
    "var_of_prediction = dist.Bernoulli(out.mean(0)).variance\n",
    "\n",
    "# Epistemic uncertainty: Variance across values of y_hat\n",
    "var_of_mean = out.var(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330d3f0",
   "metadata": {},
   "source": [
    "### Darker colors are more uncertain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8588dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"All uncertainty ($Var[y | x]$)\")\n",
    "plt.contourf(XX, YY, var_of_prediction.reshape(XX.shape), cmap='Blues', vmin=0, vmax=0.3)\n",
    "plt.colorbar();\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='autumn', edgecolor='k');\n",
    "# - torch.sigmoid(out).var(0).reshape(XX.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d027972",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Epistemic uncertainty only\")\n",
    "plt.contourf(XX, YY, var_of_mean.reshape(XX.shape), cmap='Blues', vmin=0, vmax=0.28)\n",
    "plt.colorbar();\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='autumn', edgecolor='k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5818ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Aleatoric uncertainty only\")\n",
    "plt.contourf(XX, YY, (var_of_prediction - var_of_mean).reshape(XX.shape), cmap='Blues', vmin=0, vmax=0.3)\n",
    "plt.colorbar();\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='autumn', edgecolor='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c4b9eb",
   "metadata": {},
   "source": [
    "# TASK 4 (up to 5 points, free response): Interpretation\n",
    "\n",
    "Take a look at Figure 1 in the paper https://arxiv.org/pdf/2002.10118.pdf, \"Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks\", Kristiadi et al. 2020.\n",
    "\n",
    "1. (4 points) Qualitatively, the uncertainty in figure 1 probably looks quite different from your plots in this notebook. Try to give two different reasons for why the results here might be different.\n",
    "2. (1 point) What is going on with calibration / temperature scaling? Why doesn't the temperature scaling help in Figure 1 in the paper, and why doesn't the ECE improve in this case when we do sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e0a5f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "907331bed8a1c977c24d055220bf1d9e",
     "grade": true,
     "grade_id": "free-form",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ddbfcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
